import cv2
from picamera2 import Picamera2
from libcamera import Transform
from gpiozero import PWMOutputDevice, DigitalOutputDevice, DistanceSensor, RGBLED, Button
from time import sleep
import asyncio
import websockets
import json
import os
import threading

# === ê¸€ë¡œë²Œ ë³€ìˆ˜ ===
event_loop = None
button_event_queue = asyncio.Queue()

# === ë²„íŠ¼ ì´ˆê¸°í™” ===
button = Button(15)  # GPIO 15ì— ë²„íŠ¼ ì—°ê²°

# === ë²„íŠ¼ ì´ë²¤íŠ¸ ì²˜ë¦¬ í•¨ìˆ˜ ===
def on_button_pressed():
    global event_loop
    if event_loop:
        asyncio.run_coroutine_threadsafe(button_event_queue.put("pressed"), event_loop)

# ë²„íŠ¼ ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ ì—°ê²°
button.when_pressed = on_button_pressed

# === ë²„íŠ¼ ì´ë²¤íŠ¸ ì²˜ë¦¬ ì½”ë£¨í‹´ ===
async def handle_button_events():
    global current_state, autonomous_task
    while True:
        event = await button_event_queue.get()
        if event == "pressed":
            if current_state == "run":
                current_state = "stop"
                print("ë²„íŠ¼ìœ¼ë¡œ ì£¼í–‰ ì •ì§€")
                if autonomous_task:
                    autonomous_task.cancel()
                stop_motors()
                rgbled.color = (1, 0, 0)  # ë¹¨ê°„ìƒ‰
            elif current_state == "stop":
                current_state = "run"
                print("ë²„íŠ¼ìœ¼ë¡œ ì£¼í–‰ ì‹œì‘")
                rgbled.color = (0, 1, 0)  # ì´ˆë¡ìƒ‰
                if autonomous_task:
                    autonomous_task.cancel()
                autonomous_task = asyncio.create_task(autonomous_driving())



# ì›¹ì†Œì¼“ ì„œë²„ ì£¼ì†Œ (ë…¸íŠ¸ë¶ì˜ IP ì£¼ì†Œì™€ í¬íŠ¸)
SERVER_IP = "192.168.137.1"  # ë…¸íŠ¸ë¶ì˜ ì‹¤ì œ IP ì£¼ì†Œë¡œ ë³€ê²½
SERVER_PORT = 8989
WEBSOCKET_SERVER_URL = f"ws://{SERVER_IP}:{SERVER_PORT}"

rgbled = RGBLED(red=5, green=6, blue=13, active_high=True) 

# === GPIO í•€ ì„¤ì • ===
PWMA = PWMOutputDevice(18)
AIN1 = DigitalOutputDevice(22) #ì™¼ ë°”í€´ í›„ì§„
AIN2 = DigitalOutputDevice(27) #ì™¼ ë°”í€´ ì „ì§„
PWMB = PWMOutputDevice(23)
BIN1 = DigitalOutputDevice(25) #ì˜¤ë¥¸ ë°”í€´ í›„ì§„
BIN2 = DigitalOutputDevice(24) #ì˜¤ë¥¸ ë°”í€´ ì „ì§„

# === ì´ˆìŒíŒŒ ì„¼ì„œ ì„¤ì • ===
front_sensor = DistanceSensor(echo=10, trigger=9, max_distance=2.0)
left_sensor = DistanceSensor(echo=17, trigger=4, max_distance=2.0)
right_sensor = DistanceSensor(echo=8, trigger=7, max_distance=2.0)

# === ìƒíƒœ ë³€ìˆ˜ ===
current_state = "stop"  # ê¸°ë³¸ ìƒíƒœëŠ” ì •ì§€
autonomous_task = None  # ììœ¨ì£¼í–‰ ì‘ì—… ì¶”ì 
last_state = "stop"  # ë§ˆì§€ë§‰ ìƒíƒœ ì¶”ì 

# === ëª¨í„° ì œì–´ í•¨ìˆ˜ ===
def stop_motors():
    PWMA.value = 0.0
    PWMB.value = 0.0

def move_forward():
    AIN1.value, AIN2.value = 0, 1
    BIN1.value, BIN2.value = 0, 1
    PWMA.value = 1
    PWMB.value = 1

def turn_left():
    AIN1.value, AIN2.value = 1, 0
    BIN1.value, BIN2.value = 0, 1
    PWMA.value = 0.6
    PWMB.value = 0.6

def turn_right():
    AIN1.value, AIN2.value = 0, 1
    BIN1.value, BIN2.value = 1, 0
    PWMA.value = 0.6
    PWMB.value = 0.6

def soft_turn_left():
    AIN1.value, AIN2.value = 0, 0
    BIN1.value, BIN2.value = 0, 1
    PWMA.value = 1
    PWMB.value = 1

def soft_turn_right():
    AIN1.value, AIN2.value = 0, 1
    BIN1.value, BIN2.value = 0, 0
    PWMA.value = 1
    PWMB.value = 1


classNames = {0: 'background',
              1: 'person', 2: 'bicycle', 3: 'car', 4: 'motorcycle', 5: 'airplane', 6: 'bus',
              7: 'train', 8: 'truck', 9: 'boat', 10: 'traffic light', 11: 'fire hydrant',
              13: 'stop sign', 14: 'parking meter', 15: 'bench', 16: 'bird', 17: 'cat',
              18: 'dog', 19: 'horse', 20: 'sheep', 21: 'cow', 22: 'elephant', 23: 'bear',
              24: 'zebra', 25: 'giraffe', 27: 'backpack', 28: 'umbrella', 31: 'handbag',
              32: 'tie', 33: 'suitcase', 34: 'frisbee', 35: 'skis', 36: 'snowboard',
              37: 'sports ball', 38: 'kite', 39: 'baseball bat', 40: 'baseball glove',
              41: 'skateboard', 42: 'surfboard', 43: 'tennis racket', 44: 'bottle',
              46: 'wine glass', 47: 'cup', 48: 'fork', 49: 'knife', 50: 'spoon',
              51: 'bowl', 52: 'banana', 53: 'apple', 54: 'sandwich', 55: 'orange',
              56: 'broccoli', 57: 'carrot', 58: 'hot dog', 59: 'pizza', 60: 'donut',
              61: 'cake', 62: 'chair', 63: 'couch', 64: 'potted plant', 65: 'bed',
              67: 'dining table', 70: 'toilet', 72: 'tv', 73: 'laptop', 74: 'mouse',
              75: 'remote', 76: 'keyboard', 77: 'cell phone', 78: 'microwave', 79: 'oven',
              80: 'toaster', 81: 'sink', 82: 'refrigerator', 84: 'book', 85: 'clock',
              86: 'vase', 87: 'scissors', 88: 'teddy bear', 89: 'hair drier', 90: 'toothbrush'}

def id_class_name(class_id, classes):
    return classes.get(class_id, "Unknown")

async def autonomous_driving():
    camera = Picamera2()
    camera.configure(camera.create_preview_configuration(
        main={"format": 'XRGB8888', "size": (640, 480)},
        transform=Transform(hflip=1, vflip=1) # ìˆ˜í‰, ìˆ˜ì§ ë’¤ì§‘ê¸° ë™ì‹œì— ì ìš© (180ë„ íšŒì „ íš¨ê³¼)
    ))
    
    try:
        camera.start()
        print("âœ… Picamera2ê°€ ì„±ê³µì ìœ¼ë¡œ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"ğŸš¨ğŸš¨ğŸš¨ ì—ëŸ¬: Picamera2ë¥¼ ì‹œì‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤! ğŸš¨ğŸš¨ğŸš¨")
        print(f"  - ì—ëŸ¬ ë©”ì‹œì§€: {e}")
        print("  1. ì¹´ë©”ë¼ ëª¨ë“ˆì´ ì œëŒ€ë¡œ ì—°ê²°ë˜ì–´ ìˆë‚˜ìš”?")
        print("  2. 'sudo raspi-config'ì—ì„œ ì¹´ë©”ë¼ ì˜µì…˜ì´ í™œì„±í™”ë˜ì–´ ìˆë‚˜ìš”?")
        print("  3. ë‹¤ë¥¸ í”„ë¡œê·¸ë¨ì´ ì¹´ë©”ë¼ë¥¼ ì‚¬ìš©í•˜ê³  ìˆì§€ëŠ” ì•Šë‚˜ìš”?")
        print("  4. ë¼ì¦ˆë² ë¦¬ íŒŒì´ë¥¼ ì¬ë¶€íŒ…í•´ë³´ì…¨ë‚˜ìš”?")
        return

    try:
        model = cv2.dnn.readNetFromTensorflow('/home/robot/market-with-following-basket/tracking/models/frozen_inference_graph.pb',
                                      '/home/robot/market-with-following-basket/tracking/models/ssd_mobilenet_v2_coco_2018_03_29.pbtxt')
        
        if model.empty():
            print("ğŸš¨ ì—ëŸ¬: DNN ëª¨ë¸ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ ê²½ë¡œì™€ íŒŒì¼ ì†ìƒ ì—¬ë¶€ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
            return
        last_state = "stop"
        while current_state == "run":
            keyValue = cv2.waitKey(1)
        
            if keyValue == ord('q') :
                break
            
            image = camera.capture_array()
            image = cv2.cvtColor(image, cv2.COLOR_BGRA2BGR)
            
            image_height, image_width, _ = image.shape

            model.setInput(cv2.dnn.blobFromImage(image, size=(300, 300), swapRB=True))
            output = model.forward()

            person_detected = False
            
            front = front_sensor.distance * 100
            left = left_sensor.distance * 100
            right = right_sensor.distance * 100
            print(f"ê±°ë¦¬ - ì •ë©´: {front:.1f}cm, ì™¼ìª½: {left:.1f}cm, ì˜¤ë¥¸ìª½: {right:.1f}cm")

            for detection in output[0, 0, :, :]:
                confidence = detection[2]
                class_id = int(detection[1])
                
                if confidence > .5 and class_id == 1: # ì‹ ë¢°ë„ê°€ 0.5 ì´ìƒì´ê³ , í´ë˜ìŠ¤ IDê°€ 1 (ì‚¬ëŒ)ì¸ ê²½ìš°
                    class_name=id_class_name(class_id,classNames)

                    person_detected = True

                    box_x_min = int(detection[3] * image_width)
                    box_y_min = int(detection[4] * image_height)
                    box_x_max = int(detection[5] * image_width)
                    box_y_max = int(detection[6] * image_height)
                    box_center_x = int((box_x_min + box_x_max) / 2)

                    cv2.rectangle(image, (box_x_min, box_y_min), (box_x_max, box_y_max), (0, 0, 200), thickness=2)
                    cv2.line(image, (box_center_x, box_y_min), (box_center_x, box_y_max), (200, 0, 0), thickness=2)
                    print(f"box_y_min: {box_y_min}, box_y_max: {box_y_max}")
                          
                    if box_y_min < 50:
                        stop_motors()
                        print("ëŒ€ìƒê³¼ ì ì •ê±°ë¦¬ ìœ ì§€")
                        rgbled.color = (0, 1, 0)  # ì´ˆë¡ìƒ‰
                    
                    elif left < 20:
                        soft_turn_right()
                        print("ì™¼ìª½ì— ì¥ì• ë¬¼ â†’ ì˜¤ë¥¸ìª½ íœ˜ì–´ì„œ ì§„í–‰")
                    
                    elif right < 20:
                        soft_turn_left()
                        print("ì˜¤ë¥¸ìª½ì— ì¥ì• ë¬¼ â†’ ì™¼ìª½ íœ˜ì–´ì„œ ì§„í–‰")

                    elif box_center_x < image_width // 2 - 40:
                        #if not obstacle_avoidance():
                            soft_turn_left()
                            print("ëª©í‘œê°€ ì™¼ìª½ì— ìˆìŒ")
                            rgbled.color = (0, 1, 0)  # ì´ˆë¡ìƒ‰
                            last_state = "left"
                    elif box_center_x > image_width // 2 + 40:
                        #if not obstacle_avoidance():
                            soft_turn_right()
                            last_state = "right"
                            rgbled.color = (0, 1, 0)  # ì´ˆë¡ìƒ‰
                            print("ëª©í‘œê°€ ì˜¤ë¥¸ìª½ì— ìˆìŒ")
                    else :
                        #if not obstacle_avoidance():
                            move_forward()
                            print("ëª©í‘œê°€ ì •ë©´ì— ìˆìŒ")
                            rgbled.color = (0, 1, 0)  # ì´ˆë¡ìƒ‰
                            sleep(1)
                    

                    text_x = box_x_min
                    text_y = box_y_min - 10 if box_y_min - 10 > 10 else box_y_min + 20 
                    text = f"{class_name}: {confidence:.2f}"
                    font_scale = 0.7
                    font_thickness = 2
                    cv2.putText(image, text, (text_x, text_y), cv2.FONT_HERSHEY_SIMPLEX, font_scale, (0, 0, 255), font_thickness)
                
            if not person_detected:
                if last_state == "left":
                    turn_left()
                    print("ë§ˆì§€ë§‰ ìœ„ì¹˜ ì™¼ìª½")
                    rgbled.color = (1, 0, 0)
                    
                elif last_state == "right":
                    turn_right()
                    print("ë§ˆì§€ë§‰ ìœ„ì¹˜ ì˜¤ë¥¸ìª½")
                    rgbled.color = (1, 0, 0)
                # stop_motors()
                # print("ì‚¬ëŒì´ ê°ì§€ë˜ì§€ ì•ŠìŒ, ì •ì§€")

            cv2.imshow('Object Detection Result', image)
            await asyncio.sleep(0.1)
                        
    except KeyboardInterrupt:
        print("\ní”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤. (Ctrl+C ê°ì§€)")
    except Exception as e:
        print(f"ğŸš¨ ì˜ˆìƒì¹˜ ëª»í•œ ì—ëŸ¬ ë°œìƒ: {e}")
    except asyncio.CancelledError:
        # CancelledError ë°œìƒ ì‹œ ì •ë¦¬ ì‘ì—…
        print("ììœ¨ì£¼í–‰ ì‘ì—… ì·¨ì†Œë¨")
        stop_motors()
        camera.close()
    finally:
        try:
            if 'camera' in locals() and camera.started:
                camera.stop()
                print("âœ… Picamera2ê°€ ì„±ê³µì ìœ¼ë¡œ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"ğŸš¨ ê²½ê³ : Picamera2 ì¤‘ì§€ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
        cv2.destroyAllWindows()


# === WebSocket ì²˜ë¦¬ ===
async def handle_websocket():
    print(f"ì›¹ì†Œì¼“ ì„œë²„ì— ì—°ê²° ì¤‘")
    global current_state, autonomous_task

    while True:
        try:
            async with websockets.connect("ws://192.168.137.1:8989") as websocket:
                print("ì›¹ì†Œì¼“ ì„œë²„ì— ì—°ê²°ë˜ì—ˆìŠµë‹ˆë‹¤.")
                while True:
                    message = await websocket.recv()
                    data = json.loads(message)
                    command = data.get("command")

                    if command == "run" and current_state != "run":
                        current_state = "run"
                        if autonomous_task:
                            autonomous_task.cancel()
                        autonomous_task = asyncio.create_task(autonomous_driving())

                    elif command == "stop" and current_state != "stop":
                        current_state = "stop"
                        if autonomous_task:
                            autonomous_task.cancel()
                        stop_motors()
                        rgbled.color = (0, 0, 0)
        except Exception as e:
            print(f"WebSocket Error: {e}")
        await asyncio.sleep(5)

# === ì´ë²¤íŠ¸ ë£¨í”„ ìŠ¤ë ˆë“œ ===
def start_event_loop():
    global event_loop
    event_loop = asyncio.new_event_loop()
    asyncio.set_event_loop(event_loop)
    event_loop.run_until_complete(asyncio.gather(
        handle_button_events(),
        handle_websocket()
    ))

# === ë©”ì¸ ===
if __name__ == "__main__":
    # ë³„ë„ì˜ ìŠ¤ë ˆë“œì—ì„œ asyncio ë£¨í”„ ì‹¤í–‰
    loop_thread = threading.Thread(target=start_event_loop, daemon=True)
    loop_thread.start()

    try:
        while True:
            pass  # ë©”ì¸ ë£¨í”„ë¥¼ ìœ ì§€í•˜ê¸° ìœ„í•œ ìë¦¬í‘œì‹œì
    except KeyboardInterrupt:
        print("í”„ë¡œê·¸ë¨ ì¢…ë£Œ")